{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"182gqmQpQ3sK22qFnkHGSfUsx7n9SFZuC","timestamp":1700132329460}],"authorship_tag":"ABX9TyMzBVnnJcZ5KLvp/VczFz31"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mental healh severity classes prediction #\n","Using a filtered datafile that has most of pre-processing already done, expecting this to result in higher accuracy models.\n","\n","**Dataset cleaning and preparation.ipynb** logic now includes\n","scores calculations, categorization, labels adding and combining of 10 TIPI questions to personality trait scores\n","\n","Also, this time trying to treat mental health condition categories as ordinal values and hoping to get better prediction results.\n","\n","\n"],"metadata":{"id":"psVaSLGCzdIB"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"CZBKlZTkhUDL","executionInfo":{"status":"ok","timestamp":1700166261708,"user_tz":-120,"elapsed":3477,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"44a20c3e-bf30-40f8-c23c-32368e4bdf15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["   Q1A  Q1I   Q1E  Q2A  Q2I   Q2E  Q3A  Q3I   Q3E  Q4A  Q4I   Q4E  Q5A  Q5I  \\\n","0    3   28  3890    3   25  2122    1   16  1944    3    8  2044    3   34   \n","1    3    2  8118    0   36  2890    1   35  4777    2   28  3090    3   10   \n","2    2    7  5784    0   33  4373    3   41  3242    0   13  6470    3   11   \n","3    1   23  5081    2   11  6837    1   37  5521    0   27  4556    2   28   \n","4    1   36  3215    1   13  7731    2    5  4156    3   10  2802    3    2   \n","\n","    Q5E  Q6A  Q6I   Q6E  Q7A  Q7I   Q7E  Q8A  Q8I   Q8E  Q9A  Q9I   Q9E  Q10A  \\\n","0  2153    3   33  2416    3   10  2818    3   13  2259    1   21  5541     0   \n","1  5078    3   40  2790    2   18  3408    3    1  8342    2   37   916     1   \n","2  3927    2    9  3704    0   17  4550    2    5  3021    1   32  5864     3   \n","3  3269    2   26  3231    3    2  7138    1   19  3079    2   31  9650     2   \n","4  5628    1    9  6522    3   34  2374    3   11  3054    3    7  2975     2   \n","\n","   Q10I  Q10E  Q11A  Q11I  Q11E  Q12A  Q12I  Q12E  Q13A  Q13I  Q13E  Q14A  \\\n","0    38  4441     3    31  2451     3    24  3325     3    14  1416     3   \n","1    32  1537     1    21  3926     1    25  3691     3    26  2004     3   \n","2    21  3722     1    10  3424     0    36  3236     3    23  2489     0   \n","3    17  4179     1     5  5928     0    21  2838     0    20  2560     3   \n","4    14  3524     1    33  3033     3    23  2132     3    17  1314     3   \n","\n","   Q14I  Q14E  Q15A  Q15I  Q15E  Q16A  Q16I  Q16E  Q17A  Q17I  Q17E  Q18A  \\\n","0    37  5021     3    27  2342     3    39  2480     2     6  2476     3   \n","1     4  8888     2    27  4109     2    19  4058     3    12  3692     1   \n","2    34  7290     3    12  6587     3    22  3627     3    38  2905     1   \n","3    29  5139     1    22  3597     1    35  3336     2    10  4506     0   \n","4    16  3181     3    26  2249     2    19  2623     3    35  3093     3   \n","\n","   Q18I  Q18E  Q19A  Q19I   Q19E  Q20A  Q20I  Q20E  Q21A  Q21I  Q21E  Q22A  \\\n","0    35  1627     2    17   9050     2    30  7001     0    11  4719     3   \n","1     6  3373     0    23   6015     0    16  3023     1    22  2670     2   \n","2    18  2998     1     8  10233     0    16  4258     3    28  2888     2   \n","3    14  2695     0    25   8128     1    15  3125     0     6  4061     0   \n","4    38  7098     3    37   1938     3    15  3502     2    32  4776     2   \n","\n","   Q22I   Q22E  Q23A  Q23I   Q23E  Q24A  Q24I  Q24E  Q25A  Q25I   Q25E  Q26A  \\\n","0    20   2984     3    36   1313     3    42  2444     3     1   9880     3   \n","1     3   5727     0    39   3641     1    33  2670     1     7   7649     2   \n","2     4  59592     1     3  11732     3     2  8834     1    29   7358     0   \n","3    40   4272     0    12   4029     0     9  5630     0    18  30631     1   \n","4    18   4463     3     4   2436     1    40  4047     3    31   3787     3   \n","\n","   Q26I  Q26E  Q27A  Q27I   Q27E  Q28A  Q28I  Q28E  Q29A  Q29I  Q29E  Q30A  \\\n","0     2  4695     3     5   1677     2     4  6723     3     3  5953     1   \n","1    11  2537     2     5   2907     3     9  1685     2    41  4726     2   \n","2    30  4928     1    15   3036     0    19  4127     1    37  3934     1   \n","3    24  9870     3     4   2411     0    16  9478     2     1  7618     2   \n","4    42  2102     1     1  12351     3     3  2410     1    22  5056     3   \n","\n","   Q30I   Q30E  Q31A  Q31I  Q31E  Q32A  Q32I  Q32E  Q33A  Q33I  Q33E  Q34A  \\\n","0    26   8062     3    12  5560     3     7  3032     1    29  3316     2   \n","1    17   6063     1    20  3307     2    14  4995     2    38  2505     1   \n","2    26  10782     3     1  8273     2    39  3501     0    27  3824     3   \n","3    32  12639     2    34  5378     0    41  8923     1    38  2977     3   \n","4    39   3343     2    27  3012     3    20  3520     3     8  1868     3   \n","\n","   Q34I  Q34E  Q35A  Q35I   Q35E  Q36A  Q36I  Q36E  Q37A  Q37I  Q37E  Q38A  \\\n","0    40  3563     3    23   5594     3    41  1477     0    18  3885     1   \n","1    34  2540     1    31   4359     2    15  3925     3    13  4609     1   \n","2    25  2141     2     6  17461     3    24  1557     3    40  4446     3   \n","3     3  5620     0     7  16760     0     8  6427     1    39  3760     0   \n","4    25  2536     2    24   3725     3    30  2130     2    29  3952     2   \n","\n","   Q38I   Q38E  Q39A  Q39I  Q39E  Q40A  Q40I  Q40E  Q41A  Q41I  Q41E  Q42A  \\\n","0     9   5265     3    19  1892     2    22  4228     3    32  1574     3   \n","1    30   3755     1    42  2323     0    24  5713     1     8  1334     1   \n","2    42   1883     1    35  5790     1    14  4432     0    20  2203     3   \n","3    13   4112     2    42  2769     3    33  4432     3    30  3643     1   \n","4    21  10694     2    41  3231     3    12  3604     3    28  1950     2   \n","\n","   Q42I  Q42E country  source  introelapse  testelapse  surveyelapse  TIPI1  \\\n","0    15  2969      IN       2           19         167           166    1.0   \n","1    29  5562      US       2            1         193           186    6.0   \n","2    31  5768      PL       2            5         271           122    2.0   \n","3    36  3698      US       2            3         261           336    1.0   \n","4     6  6265      MY       2         1766         164           157    2.0   \n","\n","   TIPI2  TIPI3  TIPI4  TIPI5  TIPI6  TIPI7  TIPI8  TIPI9  TIPI10  VCL1  VCL2  \\\n","0    3.0    7.0    1.0    7.0    1.0    7.0    3.0    1.0     7.0     1     0   \n","1    3.0    4.0    1.0    5.0    4.0    7.0    1.0    1.0     3.0     1     1   \n","2    3.0    2.0    6.0    5.0    2.0    5.0    3.0    3.0     6.0     1     0   \n","3    7.0    7.0    4.0    6.0    4.0    6.0    7.0    6.0     7.0     1     0   \n","4    3.0    3.0    2.0    5.0    3.0    5.0    2.0    3.0     5.0     1     1   \n","\n","   VCL3  VCL4  VCL5  VCL6  VCL7  VCL8  VCL9  VCL10  VCL11  VCL12  VCL13  \\\n","0     0     1     1     0     1     0     0      1      0      0      0   \n","1     0     1     1     0     0     0     0      1      0      0      0   \n","2     0     1     1     0     0     0     0      0      1      0      0   \n","3     0     1     1     0     0     0     0      1      0      0      0   \n","4     0     1     1     0     0     1     0      1      0      0      1   \n","\n","   VCL14  VCL15  VCL16  education  urban  gender  engnat  age  screensize  \\\n","0      1      1      1        2.0    3.0     2.0     2.0   16           1   \n","1      1      1      1        2.0    3.0     2.0     1.0   16           2   \n","2      1      1      1        2.0    3.0     2.0     2.0   17           2   \n","3      1      1      1        1.0    3.0     2.0     1.0   13           2   \n","4      1      1      1        3.0    2.0     2.0     2.0   19           2   \n","\n","   uniquenetworklocation  hand  religion  orientation  race  voted  married  \\\n","0                      1   1.0      12.0          1.0    10    2.0      1.0   \n","1                      1   2.0       7.0          NaN    70    2.0      1.0   \n","2                      1   1.0       4.0          3.0    60    1.0      1.0   \n","3                      1   2.0       4.0          5.0    70    2.0      1.0   \n","4                      2   3.0      10.0          1.0    10    2.0      1.0   \n","\n","   familysize       major  mean_response_time  std_deviation_dass  \\\n","0         2.0         NaN         3752.904762            0.964227   \n","1         4.0         NaN         4022.071429            0.961513   \n","2         3.0         NaN         6433.190476            1.208756   \n","3         5.0     biology         6036.452381            1.080123   \n","4         4.0  Psychology         3890.523810            0.766987   \n","\n","         education_label urban_label gender_label engnat_label hand_label  \\\n","0            High school       Urban       Female           No      Right   \n","1            High school       Urban       Female          Yes       Left   \n","2            High school       Urban       Female           No      Right   \n","3  Less than high school       Urban       Female          Yes       Left   \n","4      University degree    Suburban       Female           No       Both   \n","\n","         religion_label orientation_label race_label voted_label  \\\n","0                 Other      Heterosexual      Asian          No   \n","1     Christian (Other)               NaN      Other          No   \n","2  Christian (Catholic)        Homosexual      White         Yes   \n","3  Christian (Catholic)             Other      Other          No   \n","4                Muslim      Heterosexual      Asian          No   \n","\n","   married_label  Extraversion  Agreeableness  Conscientiousness  \\\n","0  Never married           1.0            5.0                5.0   \n","1  Never married           5.0            5.0                2.5   \n","2  Never married           2.0            4.0                2.5   \n","3  Never married           2.5            6.5                7.0   \n","4  Never married           2.5            4.0                2.5   \n","\n","   Emotional_Stability  Openness  Depression_Score  Anxiety_Score  \\\n","0                  1.0       7.0                27             34   \n","1                  1.0       4.0                24             17   \n","2                  4.5       5.5                39             12   \n","3                  5.0       6.5                16             17   \n","4                  2.5       5.0                32             40   \n","\n","   Stress_Score Depression_Severity  Anxiety_Severity   Stress_Severity  \n","0            40              Severe  Extremely Severe  Extremely Severe  \n","1            27              Severe            Severe            Severe  \n","2            17    Extremely Severe          Moderate              Mild  \n","3            16            Moderate            Severe              Mild  \n","4            29    Extremely Severe  Extremely Severe            Severe  "],"text/html":["\n","  <div id=\"df-58b548d7-647e-427c-9192-057c55066280\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q1A</th>\n","      <th>Q1I</th>\n","      <th>Q1E</th>\n","      <th>Q2A</th>\n","      <th>Q2I</th>\n","      <th>Q2E</th>\n","      <th>Q3A</th>\n","      <th>Q3I</th>\n","      <th>Q3E</th>\n","      <th>Q4A</th>\n","      <th>Q4I</th>\n","      <th>Q4E</th>\n","      <th>Q5A</th>\n","      <th>Q5I</th>\n","      <th>Q5E</th>\n","      <th>Q6A</th>\n","      <th>Q6I</th>\n","      <th>Q6E</th>\n","      <th>Q7A</th>\n","      <th>Q7I</th>\n","      <th>Q7E</th>\n","      <th>Q8A</th>\n","      <th>Q8I</th>\n","      <th>Q8E</th>\n","      <th>Q9A</th>\n","      <th>Q9I</th>\n","      <th>Q9E</th>\n","      <th>Q10A</th>\n","      <th>Q10I</th>\n","      <th>Q10E</th>\n","      <th>Q11A</th>\n","      <th>Q11I</th>\n","      <th>Q11E</th>\n","      <th>Q12A</th>\n","      <th>Q12I</th>\n","      <th>Q12E</th>\n","      <th>Q13A</th>\n","      <th>Q13I</th>\n","      <th>Q13E</th>\n","      <th>Q14A</th>\n","      <th>Q14I</th>\n","      <th>Q14E</th>\n","      <th>Q15A</th>\n","      <th>Q15I</th>\n","      <th>Q15E</th>\n","      <th>Q16A</th>\n","      <th>Q16I</th>\n","      <th>Q16E</th>\n","      <th>Q17A</th>\n","      <th>Q17I</th>\n","      <th>Q17E</th>\n","      <th>Q18A</th>\n","      <th>Q18I</th>\n","      <th>Q18E</th>\n","      <th>Q19A</th>\n","      <th>Q19I</th>\n","      <th>Q19E</th>\n","      <th>Q20A</th>\n","      <th>Q20I</th>\n","      <th>Q20E</th>\n","      <th>Q21A</th>\n","      <th>Q21I</th>\n","      <th>Q21E</th>\n","      <th>Q22A</th>\n","      <th>Q22I</th>\n","      <th>Q22E</th>\n","      <th>Q23A</th>\n","      <th>Q23I</th>\n","      <th>Q23E</th>\n","      <th>Q24A</th>\n","      <th>Q24I</th>\n","      <th>Q24E</th>\n","      <th>Q25A</th>\n","      <th>Q25I</th>\n","      <th>Q25E</th>\n","      <th>Q26A</th>\n","      <th>Q26I</th>\n","      <th>Q26E</th>\n","      <th>Q27A</th>\n","      <th>Q27I</th>\n","      <th>Q27E</th>\n","      <th>Q28A</th>\n","      <th>Q28I</th>\n","      <th>Q28E</th>\n","      <th>Q29A</th>\n","      <th>Q29I</th>\n","      <th>Q29E</th>\n","      <th>Q30A</th>\n","      <th>Q30I</th>\n","      <th>Q30E</th>\n","      <th>Q31A</th>\n","      <th>Q31I</th>\n","      <th>Q31E</th>\n","      <th>Q32A</th>\n","      <th>Q32I</th>\n","      <th>Q32E</th>\n","      <th>Q33A</th>\n","      <th>Q33I</th>\n","      <th>Q33E</th>\n","      <th>Q34A</th>\n","      <th>Q34I</th>\n","      <th>Q34E</th>\n","      <th>Q35A</th>\n","      <th>Q35I</th>\n","      <th>Q35E</th>\n","      <th>Q36A</th>\n","      <th>Q36I</th>\n","      <th>Q36E</th>\n","      <th>Q37A</th>\n","      <th>Q37I</th>\n","      <th>Q37E</th>\n","      <th>Q38A</th>\n","      <th>Q38I</th>\n","      <th>Q38E</th>\n","      <th>Q39A</th>\n","      <th>Q39I</th>\n","      <th>Q39E</th>\n","      <th>Q40A</th>\n","      <th>Q40I</th>\n","      <th>Q40E</th>\n","      <th>Q41A</th>\n","      <th>Q41I</th>\n","      <th>Q41E</th>\n","      <th>Q42A</th>\n","      <th>Q42I</th>\n","      <th>Q42E</th>\n","      <th>country</th>\n","      <th>source</th>\n","      <th>introelapse</th>\n","      <th>testelapse</th>\n","      <th>surveyelapse</th>\n","      <th>TIPI1</th>\n","      <th>TIPI2</th>\n","      <th>TIPI3</th>\n","      <th>TIPI4</th>\n","      <th>TIPI5</th>\n","      <th>TIPI6</th>\n","      <th>TIPI7</th>\n","      <th>TIPI8</th>\n","      <th>TIPI9</th>\n","      <th>TIPI10</th>\n","      <th>VCL1</th>\n","      <th>VCL2</th>\n","      <th>VCL3</th>\n","      <th>VCL4</th>\n","      <th>VCL5</th>\n","      <th>VCL6</th>\n","      <th>VCL7</th>\n","      <th>VCL8</th>\n","      <th>VCL9</th>\n","      <th>VCL10</th>\n","      <th>VCL11</th>\n","      <th>VCL12</th>\n","      <th>VCL13</th>\n","      <th>VCL14</th>\n","      <th>VCL15</th>\n","      <th>VCL16</th>\n","      <th>education</th>\n","      <th>urban</th>\n","      <th>gender</th>\n","      <th>engnat</th>\n","      <th>age</th>\n","      <th>screensize</th>\n","      <th>uniquenetworklocation</th>\n","      <th>hand</th>\n","      <th>religion</th>\n","      <th>orientation</th>\n","      <th>race</th>\n","      <th>voted</th>\n","      <th>married</th>\n","      <th>familysize</th>\n","      <th>major</th>\n","      <th>mean_response_time</th>\n","      <th>std_deviation_dass</th>\n","      <th>education_label</th>\n","      <th>urban_label</th>\n","      <th>gender_label</th>\n","      <th>engnat_label</th>\n","      <th>hand_label</th>\n","      <th>religion_label</th>\n","      <th>orientation_label</th>\n","      <th>race_label</th>\n","      <th>voted_label</th>\n","      <th>married_label</th>\n","      <th>Extraversion</th>\n","      <th>Agreeableness</th>\n","      <th>Conscientiousness</th>\n","      <th>Emotional_Stability</th>\n","      <th>Openness</th>\n","      <th>Depression_Score</th>\n","      <th>Anxiety_Score</th>\n","      <th>Stress_Score</th>\n","      <th>Depression_Severity</th>\n","      <th>Anxiety_Severity</th>\n","      <th>Stress_Severity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>28</td>\n","      <td>3890</td>\n","      <td>3</td>\n","      <td>25</td>\n","      <td>2122</td>\n","      <td>1</td>\n","      <td>16</td>\n","      <td>1944</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>2044</td>\n","      <td>3</td>\n","      <td>34</td>\n","      <td>2153</td>\n","      <td>3</td>\n","      <td>33</td>\n","      <td>2416</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>2818</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>2259</td>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>5541</td>\n","      <td>0</td>\n","      <td>38</td>\n","      <td>4441</td>\n","      <td>3</td>\n","      <td>31</td>\n","      <td>2451</td>\n","      <td>3</td>\n","      <td>24</td>\n","      <td>3325</td>\n","      <td>3</td>\n","      <td>14</td>\n","      <td>1416</td>\n","      <td>3</td>\n","      <td>37</td>\n","      <td>5021</td>\n","      <td>3</td>\n","      <td>27</td>\n","      <td>2342</td>\n","      <td>3</td>\n","      <td>39</td>\n","      <td>2480</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2476</td>\n","      <td>3</td>\n","      <td>35</td>\n","      <td>1627</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>9050</td>\n","      <td>2</td>\n","      <td>30</td>\n","      <td>7001</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>4719</td>\n","      <td>3</td>\n","      <td>20</td>\n","      <td>2984</td>\n","      <td>3</td>\n","      <td>36</td>\n","      <td>1313</td>\n","      <td>3</td>\n","      <td>42</td>\n","      <td>2444</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>9880</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>4695</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>1677</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>6723</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5953</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>8062</td>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>5560</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>3032</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>3316</td>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>3563</td>\n","      <td>3</td>\n","      <td>23</td>\n","      <td>5594</td>\n","      <td>3</td>\n","      <td>41</td>\n","      <td>1477</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>3885</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>5265</td>\n","      <td>3</td>\n","      <td>19</td>\n","      <td>1892</td>\n","      <td>2</td>\n","      <td>22</td>\n","      <td>4228</td>\n","      <td>3</td>\n","      <td>32</td>\n","      <td>1574</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>2969</td>\n","      <td>IN</td>\n","      <td>2</td>\n","      <td>19</td>\n","      <td>167</td>\n","      <td>166</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>12.0</td>\n","      <td>1.0</td>\n","      <td>10</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>3752.904762</td>\n","      <td>0.964227</td>\n","      <td>High school</td>\n","      <td>Urban</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>Right</td>\n","      <td>Other</td>\n","      <td>Heterosexual</td>\n","      <td>Asian</td>\n","      <td>No</td>\n","      <td>Never married</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>27</td>\n","      <td>34</td>\n","      <td>40</td>\n","      <td>Severe</td>\n","      <td>Extremely Severe</td>\n","      <td>Extremely Severe</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>8118</td>\n","      <td>0</td>\n","      <td>36</td>\n","      <td>2890</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>4777</td>\n","      <td>2</td>\n","      <td>28</td>\n","      <td>3090</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>5078</td>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>2790</td>\n","      <td>2</td>\n","      <td>18</td>\n","      <td>3408</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>8342</td>\n","      <td>2</td>\n","      <td>37</td>\n","      <td>916</td>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>1537</td>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>3926</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>3691</td>\n","      <td>3</td>\n","      <td>26</td>\n","      <td>2004</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>8888</td>\n","      <td>2</td>\n","      <td>27</td>\n","      <td>4109</td>\n","      <td>2</td>\n","      <td>19</td>\n","      <td>4058</td>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>3692</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>3373</td>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>6015</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>3023</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>2670</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>5727</td>\n","      <td>0</td>\n","      <td>39</td>\n","      <td>3641</td>\n","      <td>1</td>\n","      <td>33</td>\n","      <td>2670</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>7649</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>2537</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2907</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>1685</td>\n","      <td>2</td>\n","      <td>41</td>\n","      <td>4726</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>6063</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>3307</td>\n","      <td>2</td>\n","      <td>14</td>\n","      <td>4995</td>\n","      <td>2</td>\n","      <td>38</td>\n","      <td>2505</td>\n","      <td>1</td>\n","      <td>34</td>\n","      <td>2540</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>4359</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>3925</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>4609</td>\n","      <td>1</td>\n","      <td>30</td>\n","      <td>3755</td>\n","      <td>1</td>\n","      <td>42</td>\n","      <td>2323</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>5713</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>1334</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>5562</td>\n","      <td>US</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>193</td>\n","      <td>186</td>\n","      <td>6.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>NaN</td>\n","      <td>70</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>NaN</td>\n","      <td>4022.071429</td>\n","      <td>0.961513</td>\n","      <td>High school</td>\n","      <td>Urban</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Left</td>\n","      <td>Christian (Other)</td>\n","      <td>NaN</td>\n","      <td>Other</td>\n","      <td>No</td>\n","      <td>Never married</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>2.5</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>24</td>\n","      <td>17</td>\n","      <td>27</td>\n","      <td>Severe</td>\n","      <td>Severe</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>5784</td>\n","      <td>0</td>\n","      <td>33</td>\n","      <td>4373</td>\n","      <td>3</td>\n","      <td>41</td>\n","      <td>3242</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>6470</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>3927</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>3704</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>4550</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3021</td>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>5864</td>\n","      <td>3</td>\n","      <td>21</td>\n","      <td>3722</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>3424</td>\n","      <td>0</td>\n","      <td>36</td>\n","      <td>3236</td>\n","      <td>3</td>\n","      <td>23</td>\n","      <td>2489</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>7290</td>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>6587</td>\n","      <td>3</td>\n","      <td>22</td>\n","      <td>3627</td>\n","      <td>3</td>\n","      <td>38</td>\n","      <td>2905</td>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>2998</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>10233</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>4258</td>\n","      <td>3</td>\n","      <td>28</td>\n","      <td>2888</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>59592</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>11732</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>8834</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>7358</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>4928</td>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>3036</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>4127</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>3934</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>10782</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>8273</td>\n","      <td>2</td>\n","      <td>39</td>\n","      <td>3501</td>\n","      <td>0</td>\n","      <td>27</td>\n","      <td>3824</td>\n","      <td>3</td>\n","      <td>25</td>\n","      <td>2141</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>17461</td>\n","      <td>3</td>\n","      <td>24</td>\n","      <td>1557</td>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>4446</td>\n","      <td>3</td>\n","      <td>42</td>\n","      <td>1883</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>5790</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>4432</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>2203</td>\n","      <td>3</td>\n","      <td>31</td>\n","      <td>5768</td>\n","      <td>PL</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>271</td>\n","      <td>122</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>60</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>6433.190476</td>\n","      <td>1.208756</td>\n","      <td>High school</td>\n","      <td>Urban</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>Right</td>\n","      <td>Christian (Catholic)</td>\n","      <td>Homosexual</td>\n","      <td>White</td>\n","      <td>Yes</td>\n","      <td>Never married</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>4.5</td>\n","      <td>5.5</td>\n","      <td>39</td>\n","      <td>12</td>\n","      <td>17</td>\n","      <td>Extremely Severe</td>\n","      <td>Moderate</td>\n","      <td>Mild</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>5081</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>6837</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>5521</td>\n","      <td>0</td>\n","      <td>27</td>\n","      <td>4556</td>\n","      <td>2</td>\n","      <td>28</td>\n","      <td>3269</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>3231</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>7138</td>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>3079</td>\n","      <td>2</td>\n","      <td>31</td>\n","      <td>9650</td>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>4179</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>5928</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>2838</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>2560</td>\n","      <td>3</td>\n","      <td>29</td>\n","      <td>5139</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>3597</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>3336</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>4506</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>2695</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>8128</td>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>3125</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>4061</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>4272</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>4029</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5630</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>30631</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>9870</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2411</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>9478</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>7618</td>\n","      <td>2</td>\n","      <td>32</td>\n","      <td>12639</td>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>5378</td>\n","      <td>0</td>\n","      <td>41</td>\n","      <td>8923</td>\n","      <td>1</td>\n","      <td>38</td>\n","      <td>2977</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5620</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>16760</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>6427</td>\n","      <td>1</td>\n","      <td>39</td>\n","      <td>3760</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>4112</td>\n","      <td>2</td>\n","      <td>42</td>\n","      <td>2769</td>\n","      <td>3</td>\n","      <td>33</td>\n","      <td>4432</td>\n","      <td>3</td>\n","      <td>30</td>\n","      <td>3643</td>\n","      <td>1</td>\n","      <td>36</td>\n","      <td>3698</td>\n","      <td>US</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>261</td>\n","      <td>336</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","      <td>7.0</td>\n","      <td>6.0</td>\n","      <td>7.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>70</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>biology</td>\n","      <td>6036.452381</td>\n","      <td>1.080123</td>\n","      <td>Less than high school</td>\n","      <td>Urban</td>\n","      <td>Female</td>\n","      <td>Yes</td>\n","      <td>Left</td>\n","      <td>Christian (Catholic)</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>No</td>\n","      <td>Never married</td>\n","      <td>2.5</td>\n","      <td>6.5</td>\n","      <td>7.0</td>\n","      <td>5.0</td>\n","      <td>6.5</td>\n","      <td>16</td>\n","      <td>17</td>\n","      <td>16</td>\n","      <td>Moderate</td>\n","      <td>Severe</td>\n","      <td>Mild</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>36</td>\n","      <td>3215</td>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>7731</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>4156</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>2802</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>5628</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>6522</td>\n","      <td>3</td>\n","      <td>34</td>\n","      <td>2374</td>\n","      <td>3</td>\n","      <td>11</td>\n","      <td>3054</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>2975</td>\n","      <td>2</td>\n","      <td>14</td>\n","      <td>3524</td>\n","      <td>1</td>\n","      <td>33</td>\n","      <td>3033</td>\n","      <td>3</td>\n","      <td>23</td>\n","      <td>2132</td>\n","      <td>3</td>\n","      <td>17</td>\n","      <td>1314</td>\n","      <td>3</td>\n","      <td>16</td>\n","      <td>3181</td>\n","      <td>3</td>\n","      <td>26</td>\n","      <td>2249</td>\n","      <td>2</td>\n","      <td>19</td>\n","      <td>2623</td>\n","      <td>3</td>\n","      <td>35</td>\n","      <td>3093</td>\n","      <td>3</td>\n","      <td>38</td>\n","      <td>7098</td>\n","      <td>3</td>\n","      <td>37</td>\n","      <td>1938</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>3502</td>\n","      <td>2</td>\n","      <td>32</td>\n","      <td>4776</td>\n","      <td>2</td>\n","      <td>18</td>\n","      <td>4463</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2436</td>\n","      <td>1</td>\n","      <td>40</td>\n","      <td>4047</td>\n","      <td>3</td>\n","      <td>31</td>\n","      <td>3787</td>\n","      <td>3</td>\n","      <td>42</td>\n","      <td>2102</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>12351</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2410</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>5056</td>\n","      <td>3</td>\n","      <td>39</td>\n","      <td>3343</td>\n","      <td>2</td>\n","      <td>27</td>\n","      <td>3012</td>\n","      <td>3</td>\n","      <td>20</td>\n","      <td>3520</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>1868</td>\n","      <td>3</td>\n","      <td>25</td>\n","      <td>2536</td>\n","      <td>2</td>\n","      <td>24</td>\n","      <td>3725</td>\n","      <td>3</td>\n","      <td>30</td>\n","      <td>2130</td>\n","      <td>2</td>\n","      <td>29</td>\n","      <td>3952</td>\n","      <td>2</td>\n","      <td>21</td>\n","      <td>10694</td>\n","      <td>2</td>\n","      <td>41</td>\n","      <td>3231</td>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>3604</td>\n","      <td>3</td>\n","      <td>28</td>\n","      <td>1950</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>6265</td>\n","      <td>MY</td>\n","      <td>2</td>\n","      <td>1766</td>\n","      <td>164</td>\n","      <td>157</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>19</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>10</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>Psychology</td>\n","      <td>3890.523810</td>\n","      <td>0.766987</td>\n","      <td>University degree</td>\n","      <td>Suburban</td>\n","      <td>Female</td>\n","      <td>No</td>\n","      <td>Both</td>\n","      <td>Muslim</td>\n","      <td>Heterosexual</td>\n","      <td>Asian</td>\n","      <td>No</td>\n","      <td>Never married</td>\n","      <td>2.5</td>\n","      <td>4.0</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>32</td>\n","      <td>40</td>\n","      <td>29</td>\n","      <td>Extremely Severe</td>\n","      <td>Extremely Severe</td>\n","      <td>Severe</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58b548d7-647e-427c-9192-057c55066280')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-58b548d7-647e-427c-9192-057c55066280 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-58b548d7-647e-427c-9192-057c55066280');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-aede5fa7-3440-46ba-8f40-6a3232fd9d10\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aede5fa7-3440-46ba-8f40-6a3232fd9d10')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-aede5fa7-3440-46ba-8f40-6a3232fd9d10 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","\n","# This setting will print out more data for better exploration\n","pd.set_option('display.max_columns', 200)\n","pd.set_option('display.max_rows', 200)\n","\n","\n","### DASS DATASET ###\n","\n","# Read in the dataframe\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/My Drive/Data Science/DASS dataset/filtered_data_newfeatures.csv'\n","df = pd.read_csv(file_path, delimiter='\\t')\n","\n","df.head()"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from imblearn.over_sampling import SMOTE\n","\n","# Load the dataframe\n","df = pd.read_csv('/content/drive/My Drive/Data Science/DASS dataset/filtered_data_newfeatures.csv', delimiter='\\t')\n","\n","# Define columns\n","demographic_categorical_cols = [col + '_label' for col in ['education', 'urban', 'gender', 'engnat', 'hand', 'religion', 'orientation', 'race', 'voted', 'married']]\n","demographic_numeric_cols = ['age', 'familysize']\n","personality_traits = ['Extraversion', 'Agreeableness', 'Conscientiousness', 'Emotional_Stability', 'Openness']\n","\n","# Keep only the necessary columns\n","necessary_columns = demographic_categorical_cols + demographic_numeric_cols + personality_traits + ['Depression_Severity']\n","df = df[necessary_columns]\n","\n","# Mode imputation for empty categorical columns\n","imputer = SimpleImputer(strategy='most_frequent')\n","df[demographic_categorical_cols] = imputer.fit_transform(df[demographic_categorical_cols])\n","\n","# Impute missing values in numerical columns\n","num_imputer = SimpleImputer(strategy='median')\n","df[demographic_numeric_cols + personality_traits] = num_imputer.fit_transform(df[demographic_numeric_cols + personality_traits])\n","\n","# Apply one-hot encoding to categorical demographic columns\n","df = pd.get_dummies(df, columns=demographic_categorical_cols)\n","\n","# Ordinal encoding for 'Depression_Severity'\n","depression_severity_mapping = {'Normal': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Extremely Severe': 4}\n","df['Depression_Severity'] = df['Depression_Severity'].map(depression_severity_mapping)\n","\n","# Scale the numeric columns\n","scaler = StandardScaler()\n","df[demographic_numeric_cols + personality_traits] = scaler.fit_transform(df[demographic_numeric_cols + personality_traits])\n","\n","# Split the dataset into features and target variable\n","X = df.drop(columns=['Depression_Severity'])\n","y = df['Depression_Severity']\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Apply SMOTE to the training data\n","smote = SMOTE()\n","X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n","\n","# Train the Logistic Regression model on the SMOTE data\n","log_reg_model = LogisticRegression(C=1, max_iter=100, solver='liblinear')\n","log_reg_model.fit(X_train_smote, y_train_smote)\n","\n","# Make predictions and evaluate the model\n","predictions = log_reg_model.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Logistic Regression Accuracy (Depression): {accuracy:.2f}\")\n","\n","# Print the classification report\n","print(\"Classification Report:\")\n","print(classification_report(y_test, predictions))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0t0j-lFmPl1","executionInfo":{"status":"ok","timestamp":1700168111763,"user_tz":-120,"elapsed":8824,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"a00f7fb3-46fc-4565-9e68-53b817f68768"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy (Depression): 0.45\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.47      0.72      0.57      1721\n","           1       0.10      0.01      0.02       713\n","           2       0.25      0.11      0.15      1389\n","           3       0.16      0.02      0.04      1303\n","           4       0.48      0.80      0.60      2541\n","\n","    accuracy                           0.45      7667\n","   macro avg       0.29      0.33      0.27      7667\n","weighted avg       0.35      0.45      0.36      7667\n","\n"]}]},{"cell_type":"markdown","source":["Next: Checking if there is any point to use hyperparameter tuning"],"metadata":{"id":"6YJo4LZe64xt"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","# Define the parameter grid for hyperparameter tuning\n","param_grid = {\n","    'C': [0.1, 1, 10],  # Moderate range, focusing on values that prevent overfitting but not too strong\n","    'solver': ['lbfgs', 'liblinear', 'saga'],  # Good solvers for large datasets and different data distributions\n","    'max_iter': [100, 300]  # Default is often 100, but given dataset size, you might need a bit more\n","}\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression()\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# Fit GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters and best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Score:\", grid_search.best_score_)\n","\n","# Evaluate the best model on the test set\n","best_model = grid_search.best_estimator_\n","test_accuracy = best_model.score(X_test, y_test)\n","print(\"Test Set Accuracy:\", test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsPhbO8Bygun","executionInfo":{"status":"ok","timestamp":1700167045057,"user_tz":-120,"elapsed":354387,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"1c2dfeac-b840-488d-8e1e-026e9b2cffa9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}\n","Best Score: 0.45715117418684226\n","Test Set Accuracy: 0.45402373809834357\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"markdown","source":["Conclusion: Logistic regression model with standard parameters is giving decent results. Hyperparameter tuning or using a different model (eg random forest) is not improving results too much. So for anxiety and stress predicting, will just use standard Logistic Regression model.\n","\n"],"metadata":{"id":"1JgR0kgy3MSW"}},{"cell_type":"markdown","source":["Next: LightGBM with standard parameters"],"metadata":{"id":"0oLXfNOK7qQ1"}},{"cell_type":"code","source":["import lightgbm as lgb\n","\n","lgb_model = lgb.LGBMClassifier()\n","\n","# Train the model on the SMOTE data\n","lgb_model.fit(X_train_smote, y_train_smote)\n","\n","# Make predictions on the test set\n","lgb_predictions = lgb_model.predict(X_test)\n","\n","# Calculate and print the accuracy\n","lgb_accuracy = accuracy_score(y_test, lgb_predictions)\n","print(f\"LightGBM Accuracy (Depression): {lgb_accuracy:.2f}\")\n","\n","# Print the classification report\n","print(\"LightGBM Classification Report:\")\n","print(classification_report(y_test, lgb_predictions))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdV7Npll7fvk","executionInfo":{"status":"ok","timestamp":1700168379204,"user_tz":-120,"elapsed":15064,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"fa998ff8-e8f0-47b0-b724-ff595ca1460d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021054 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1869\n","[LightGBM] [Info] Number of data points in the train set: 52070, number of used features: 50\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","LightGBM Accuracy (Depression): 0.45\n","LightGBM Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.51      0.65      0.57      1721\n","           1       0.22      0.05      0.08       713\n","           2       0.26      0.20      0.22      1389\n","           3       0.20      0.04      0.07      1303\n","           4       0.49      0.77      0.60      2541\n","\n","    accuracy                           0.45      7667\n","   macro avg       0.34      0.34      0.31      7667\n","weighted avg       0.38      0.45      0.39      7667\n","\n"]}]},{"cell_type":"markdown","source":["Next: LightGBM with best parameters found via hyperparameter tuning (in previous dataset, might not be 100% useful, but"],"metadata":{"id":"YOHre8l28XOO"}},{"cell_type":"code","source":["params = {'learning_rate': 0.01, 'n_estimators': 200, 'num_leaves': 31}\n","\n","lgb_model = lgb.LGBMClassifier(**params)\n","\n","# Train the model on the SMOTE data\n","lgb_model.fit(X_train_smote, y_train_smote)\n","\n","# Make predictions on the test set\n","lgb_predictions = lgb_model.predict(X_test)\n","\n","# Calculate and print the accuracy\n","lgb_accuracy = accuracy_score(y_test, lgb_predictions)\n","print(f\"LightGBM Accuracy (Depression): {lgb_accuracy:.2f}\")\n","\n","# Print the classification report\n","print(\"LightGBM Classification Report:\")\n","print(classification_report(y_test, lgb_predictions))"],"metadata":{"id":"1DywzdeW36YW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700168480121,"user_tz":-120,"elapsed":13350,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"72ba3c5f-4848-4142-bc6b-36c6a60f458d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021029 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1869\n","[LightGBM] [Info] Number of data points in the train set: 52070, number of used features: 50\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","[LightGBM] [Info] Start training from score -1.609438\n","LightGBM Accuracy (Depression): 0.45\n","LightGBM Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.51      0.64      0.56      1721\n","           1       0.17      0.05      0.07       713\n","           2       0.26      0.23      0.24      1389\n","           3       0.23      0.04      0.07      1303\n","           4       0.51      0.76      0.61      2541\n","\n","    accuracy                           0.45      7667\n","   macro avg       0.34      0.34      0.31      7667\n","weighted avg       0.38      0.45      0.39      7667\n","\n"]}]},{"cell_type":"markdown","source":["Accuracy 0.45\n","\n","Conclusion: Despite having ordinally encoded severity classes and TIPI questions grouped into actual personality traits in this dataset, not achieving results comparable with previous notebook (where using filtered dataset without any feature engineering)."],"metadata":{"id":"tLxSV3TL_no-"}},{"cell_type":"markdown","source":["# \"Ordinal logistic regression\" model that is meant specifically for ordinal data preciction #"],"metadata":{"id":"SoKY5S1o9d9i"}},{"cell_type":"code","source":["!pip install mord"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daCN58lS7_3O","executionInfo":{"status":"ok","timestamp":1700168706036,"user_tz":-120,"elapsed":14263,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"15d15bfc-e442-4d88-cc76-ca1047ab633d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mord\n","  Downloading mord-0.7.tar.gz (8.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mord\n","  Building wheel for mord (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mord: filename=mord-0.7-py3-none-any.whl size=9886 sha256=589cdfca8c7f345a74bb992e2c9558845279cf21859c2b9c8b1d558d572c9ec5\n","  Stored in directory: /root/.cache/pip/wheels/77/00/19/3cea86fbfc737ec4acb515cd94497dcc33f943fa157548b96c\n","Successfully built mord\n","Installing collected packages: mord\n","Successfully installed mord-0.7\n"]}]},{"cell_type":"code","source":["import mord as m\n","\n","ordinal_model = m.LogisticAT(alpha=1.0)  # alpha parameter set to 1.0 as an example\n","ordinal_model.fit(X_train, y_train)\n","\n","# Make predictions and evaluate the model\n","predictions = ordinal_model.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f\"Ordinal Logistic Regression Accuracy (Depression): {accuracy:.2f}\")\n","\n","# Print the classification report\n","print(\"Ordinal Logistic Regression Classification Report:\")\n","print(classification_report(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkqDDfN28252","executionInfo":{"status":"ok","timestamp":1700168821801,"user_tz":-120,"elapsed":27652,"user":{"displayName":"TÃµnn Sikk","userId":"10547085723891276182"}},"outputId":"94a41139-6723-41f5-b1e0-ff1d90702bb0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Ordinal Logistic Regression Accuracy (Depression): 0.37\n","Ordinal Logistic Regression Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.34      0.45      1721\n","           1       0.15      0.17      0.16       713\n","           2       0.23      0.33      0.27      1389\n","           3       0.22      0.35      0.27      1303\n","           4       0.64      0.49      0.56      2541\n","\n","    accuracy                           0.37      7667\n","   macro avg       0.38      0.34      0.34      7667\n","weighted avg       0.45      0.37      0.40      7667\n","\n"]}]},{"cell_type":"markdown","source":["Accuracy score 0.37 (actually tried also hyperparameter tuning, but got nowhere close as ~0.49 as LightGBM)"],"metadata":{"id":"QotegZ_2ErZT"}},{"cell_type":"markdown","source":["*Conclusion*: Despite having two advancements, the new datafile didn't increase model performance"],"metadata":{"id":"LsADB1vFERp8"}},{"cell_type":"code","source":[],"metadata":{"id":"92nKIc4kCz7H"},"execution_count":null,"outputs":[]}]}